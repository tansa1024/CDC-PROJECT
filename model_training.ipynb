{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "153289db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, tabular_input_dim):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        self.cnn = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        cnn_out_features = self.cnn.fc.in_features\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(tabular_input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(512 + 64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, tabular):\n",
    "        img_feats = self.cnn(image)\n",
    "        tab_feats = self.mlp(tabular)\n",
    "        combined = torch.cat((img_feats, tab_feats), dim=1)\n",
    "        return self.fusion(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0fd4bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_ds = MultimodalDataset(\"data/processed_train.csv\", \"house_images\", transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce4ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{int(row['id'])}.jpg\")\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        tabular = torch.tensor(row.drop(['id', 'price_log']).values, dtype=torch.float)\n",
    "        target = torch.tensor(row['price_log'], dtype=torch.float).unsqueeze(0)\n",
    "        \n",
    "        return image, tabular, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981bc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]: 100%|██████████| 402/402 [02:45<00:00,  2.43it/s, loss=0.18]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete. Average Loss: 12.8402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/20]: 100%|██████████| 402/402 [01:26<00:00,  4.62it/s, loss=0.000141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 complete. Average Loss: 0.1256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/20]: 100%|██████████| 402/402 [01:26<00:00,  4.64it/s, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 complete. Average Loss: 0.0772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/20]: 100%|██████████| 402/402 [01:32<00:00,  4.33it/s, loss=0.21]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 complete. Average Loss: 0.0576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/20]: 100%|██████████| 402/402 [01:26<00:00,  4.65it/s, loss=0.0357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 complete. Average Loss: 0.0473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/20]: 100%|██████████| 402/402 [01:28<00:00,  4.56it/s, loss=0.356] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 complete. Average Loss: 0.0427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/20]: 100%|██████████| 402/402 [01:29<00:00,  4.48it/s, loss=0.673] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 complete. Average Loss: 0.0413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/20]: 100%|██████████| 402/402 [01:30<00:00,  4.45it/s, loss=0.00281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 complete. Average Loss: 0.0409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/20]: 100%|██████████| 402/402 [01:36<00:00,  4.15it/s, loss=0.0317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 complete. Average Loss: 0.0329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/20]: 100%|██████████| 402/402 [01:30<00:00,  4.42it/s, loss=0.159] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 complete. Average Loss: 0.0311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/20]: 100%|██████████| 402/402 [01:43<00:00,  3.89it/s, loss=0.046]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 complete. Average Loss: 0.0307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/20]: 100%|██████████| 402/402 [01:49<00:00,  3.66it/s, loss=0.346]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 complete. Average Loss: 0.0319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/20]: 100%|██████████| 402/402 [02:25<00:00,  2.77it/s, loss=0.364] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 complete. Average Loss: 0.0332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [14/20]: 100%|██████████| 402/402 [01:32<00:00,  4.36it/s, loss=0.213] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 complete. Average Loss: 0.0329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [15/20]: 100%|██████████| 402/402 [01:25<00:00,  4.68it/s, loss=0.282] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 complete. Average Loss: 0.0294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [16/20]: 100%|██████████| 402/402 [01:25<00:00,  4.70it/s, loss=0.0432] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 complete. Average Loss: 0.0279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [17/20]: 100%|██████████| 402/402 [01:32<00:00,  4.35it/s, loss=0.00114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 complete. Average Loss: 0.0267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [18/20]: 100%|██████████| 402/402 [01:34<00:00,  4.26it/s, loss=0.0774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 complete. Average Loss: 0.0247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [19/20]: 100%|██████████| 402/402 [02:00<00:00,  3.34it/s, loss=0.0389] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 complete. Average Loss: 0.0274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [20/20]: 100%|██████████| 402/402 [01:29<00:00,  4.50it/s, loss=0.0717] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 complete. Average Loss: 0.0255\n",
      "Model saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiModalModel(tabular_input_dim=17).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, total=len(train_loader), leave=True)\n",
    "    loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "\n",
    "    for images, tabular, targets in loop:\n",
    "        images, tabular, targets = images.to(device), tabular.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, tabular)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} complete. Average Loss: {avg_loss:.4f}\")\n",
    "torch.save(model.state_dict(), \"data/multimodal_model.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be5471b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 101/101 [00:56<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Performance ---\n",
      "RMSE: $166,183.80\n",
      "R² Score: 0.7639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "test_ds = MultimodalDataset(\"data/processed_test.csv\", \"house_images\", transform)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, tabular, targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        images, tabular = images.to(device), tabular.to(device)\n",
    "        \n",
    "        outputs = model(images, tabular)\n",
    "        \n",
    "        all_preds.extend(outputs.cpu().numpy())\n",
    "        all_targets.extend(targets.numpy())\n",
    "\n",
    "real_preds = np.expm1(all_preds)\n",
    "real_targets = np.expm1(all_targets)\n",
    "rmse = np.sqrt(mean_squared_error(real_targets, real_preds))\n",
    "r2 = r2_score(real_targets, real_preds)\n",
    "\n",
    "print(f\"\\n--- Model Performance ---\")\n",
    "print(f\"RMSE: ${rmse:,.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fa73d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting 5204 Houses: 100%|██████████| 169/169 [00:09<00:00, 17.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 5404 predictions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import joblib\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. SETUP & LOAD MODEL\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scaler = joblib.load('data/scaler.pkl')\n",
    "\n",
    "# Re-initialize the model and load your 75% accuracy weights\n",
    "model = MultiModalModel(tabular_input_dim=17).to(device)\n",
    "model.load_state_dict(torch.load(\"data/multimodal_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 2. LOAD AND PREPARE TEST2 DATA\n",
    "test2_df = pd.read_excel(\"test2.xlsx\")\n",
    "\n",
    "# Same engineering logic as training\n",
    "test2_df['house_age'] = 2025 - test2_df['yr_built']\n",
    "test2_df['is_renovated'] = (test2_df['yr_renovated'] > 0).astype(int)\n",
    "test2_df['sqft_lot_log'] = np.log1p(test2_df['sqft_lot'])\n",
    "\n",
    "features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot_log', 'floors', 'waterfront', \n",
    "            'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'sqft_living15', \n",
    "            'sqft_lot15', 'lat', 'long', 'house_age', 'is_renovated']\n",
    "\n",
    "# Scale features (use the scaler from training!)\n",
    "X_test2_scaled = scaler.transform(test2_df[features])\n",
    "\n",
    "# 3. ROBUST DATASET CLASS\n",
    "class FinalInferenceDataset(Dataset):\n",
    "    def __init__(self, df, scaled_features, img_dir, transform):\n",
    "        self.df = df\n",
    "        self.features = torch.tensor(scaled_features, dtype=torch.float)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = int(self.df.iloc[idx]['id'])\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_id}.jpg\")\n",
    "        \n",
    "        # Check if file exists to prevent FileNotFoundError\n",
    "        if os.path.exists(img_path):\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        else:\n",
    "            # Fallback: Blank image (zeros) so the Tabular branch can still work\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "            \n",
    "        return image, self.features[idx], img_id\n",
    "\n",
    "# 4. PREDICTION LOOP\n",
    "inference_ds = FinalInferenceDataset(test2_df, X_test2_scaled, \"house_images\", transform)\n",
    "inference_loader = DataLoader(inference_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "all_ids, all_preds = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, tabs, house_ids in tqdm(inference_loader, desc=\"Predicting 5204 Houses\"):\n",
    "        imgs, tabs = imgs.to(device), tabs.to(device)\n",
    "        \n",
    "        outputs = model(imgs, tabs)\n",
    "        \n",
    "        # Reverse log transform: exp(x) - 1\n",
    "        real_prices = np.expm1(outputs.cpu().numpy())\n",
    "        \n",
    "        all_preds.extend(real_prices.flatten())\n",
    "        all_ids.extend(house_ids.numpy())\n",
    "\n",
    "# 5. SAVE SUBMISSION\n",
    "submission = pd.DataFrame({'id': all_ids, 'predicted_price': all_preds})\n",
    "submission.to_csv(\"final_predictions.csv\", index=False)\n",
    "\n",
    "print(f\"Successfully generated {len(submission)} predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "def generate_gradcam(model, img_tensor, tab_tensor):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Target the last convolutional layer in your vision branch \n",
    "    target_layer = model.vision_features[-1][-1].bn2 \n",
    "    \n",
    "    # 2. Hooks to capture gradients and activations\n",
    "    gradients = []\n",
    "    activations = []\n",
    "    def save_gradient(grad): gradients.append(grad)\n",
    "    def save_activation(module, input, output): activations.append(output)\n",
    "    \n",
    "    target_layer.register_forward_hook(save_activation)\n",
    "    \n",
    "    # 3. Forward pass\n",
    "    output = model(img_tensor.unsqueeze(0), tab_tensor.unsqueeze(0))\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # 4. Backward pass to get gradients\n",
    "    output.backward()\n",
    "    target_layer.weight.register_hook(save_gradient)\n",
    "    \n",
    "    # 5. Calculate weights and heatmap\n",
    "    weights = torch.mean(gradients[0], dim=(2, 3), keepdim=True)\n",
    "    cam = torch.sum(weights * activations[0], dim=1).squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    # 6. Normalize and resize\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cv2.resize(cam, (224, 224))\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "    return cam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
